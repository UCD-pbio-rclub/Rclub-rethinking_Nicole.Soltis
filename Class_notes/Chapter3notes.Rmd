---
title: "Chapter3notes"
author: "Nicole E Soltis"
date: "February 19, 2016"
output: 
  html_document: 
    keep_md: yes
---
##Example 3.1
```{r}
PrPV <- 0.95
PrPM <- 0.01
PrV <- 0.001
PrP <- PrPV*PrV + PrPM*(1-PrV)
( PrVP <- PrPV*PrV / PrP )
```
## 3.2
```{r}
p_grid <- seq( from=0 , to=1 , length.out=1000 )
prior <- rep( 1 , 1000 )
likelihood <- dbinom( 6 , size=9 , prob=p_grid )
posterior <- likelihood * prior
posterior <- posterior / sum(posterior)
```
## 3.3 random sampling with replacement from the vector p_grid
```{r}
samples <- sample( p_grid , prob=posterior , size=1e4 , replace=TRUE )
```
if not including prob=posterior, will sample randomly

## 3.4
```{r}
plot(samples)
```

## 3.5
```{r}
library(rethinking)
dens(samples)
```

## 3.6
add up posterior probability where p < 0.5 
```{r}
sum( posterior[ p_grid < 0.5 ] )
```

## 3.7
```{r}
sum( samples < 0.5 ) / 1e4
```

## 3.8
```{r}
sum( samples > 0.5 & samples < 0.75 ) / 1e4
```

## 3.9
```{r}
quantile( samples , 0.8 )
```
80% of the data is below this cutoff

## 3.10
```{r}
quantile( samples , c( 0.1 , 0.9 ) )
```

## 3.11
```{r}
p_grid <- seq( from=0 , to=1 , length.out=1000 )
prior <- rep(1,1000)
likelihood <- dbinom( 3 , size=3 , prob=p_grid )
posterior <- likelihood * prior
posterior <- posterior / sum(posterior)
samples <- sample( p_grid , size=1e4 , replace=TRUE , prob=posterior )
```

## 3.12
```{r}
PI( samples , prob=0.5 )
```
percentile interval

## 3.13
```{r}
HPDI( samples , prob=0.5 )
```
highest posterior density interval

narrowest interval

if data is normal or perfectly symmetrical, HPDI is equal to PI

more different if data is skewed

## 3.14
```{r}
p_grid[ which.max(posterior) ]
```

## 3.15
```{r}
chainmode( samples , adj=0.01 )
```

## 3.16
```{r}
mean( samples )
median( samples )
```

## 3.17
```{r}
sum( posterior*abs( 0.5 - p_grid ) )
```

##3.18
```{r}
#d is set to one value of p_grid vector at a time
#and for each value of d, calculates a new vector of difference between d and ALL of p_grid
#figuring out loss for EVERY POSSIBLE decision, d
loss <- sapply( p_grid , function(d) sum( posterior*abs( d - p_grid ) ) )
plot(p_grid,loss)
which.min(loss)
p_grid[which.min(loss)]
```

sapply: take each item in (vector or list), apply function to it

similar to pulling a function down a column in excel
```{r}
square_a_number <- function(x) x*x
sapply(p_grid, square_a_number)

#quicker version
sapply(p_grid, function(x) x*x)
```

##3.19
```{r}
p_grid[ which.min(loss) ]
```

## 3.20
```{r}
dbinom( 0:2 , size=2 , prob=0.7 )
```

## 3.21

Dummy data observation of seeing W in 2 tosses of globe
```{r}
rbinom( 1 , size=2 , prob=0.7 )
```

## 3.22 

Run 10 simulations
```{r}
set.seed(100)
rbinom( 10 , size=2 , prob=0.7 )
```

## 3.23

In 100k observations, each value occurs in proportion to its likelihood
```{r}
dummy_w <- rbinom( 1e5 , size=2 , prob=0.7 )
table(dummy_w)/1e5
```

## 3.24
now with a 9-toss sample size
```{r}
library(rethinking)
dummy_w <- rbinom( 1e5 , size=9 , prob=0.7 )
simplehist( dummy_w , xlab="dummy water count" )
```

## 3.25
simulate 10k predicted observations for 9 tosses
```{r}
w <- rbinom( 1e4 , size=9 , prob=0.6 )
library(rethinking)
simplehist(w)
```
next, propagate parameter uncertainty

replace prob with samples from the posterior

## 3.26
```{r}
#from 3.11
p_grid <- seq( from=0 , to=1 , length.out=1000 )
prior <- rep(1,1000)
likelihood <- dbinom( 3 , size=3 , prob=p_grid )
posterior <- likelihood * prior
posterior <- posterior / sum(posterior)
samples <- sample( p_grid , size=1e4 , replace=TRUE , prob=posterior )
w <- rbinom( 1e4 , size=9 , prob=samples )
library(rethinking)
simplehist(w)
```

## 3.28
```{r}
library(rethinking)
data(homeworkch3)

sum(birth1) + sum(birth2)
```

## class notes
what's the point of all this?

observations

prior (some assumptions and/ or previous observations)

likelihood

posterior (distribution of parameter estimates)

sampling

describe distribution of a population

iteratively incorporate information from modeling to improve your estimation/ modeling

estimate structure of your population/ data set

incorporate previous knowledge

understand whether your model works

estimate certainty in your model/ estimation

1. develop a model that fits our data (to the extent possible)

2. use the model to estimate parameters (or principles)

3. make predictions and compare to other situations/ datasets. Describe data in ways that are not purely numeric.

## what are the steps/ procedures in relation to these goals?

1. develop a model: bayes equation, likelihood based on data (observations) that we have, and prior assumptions

a. use likelihood function and prior information to model data

b. prior = flat (1 for all values)

or prior = nonflat (1 for >0.5, 0 for <0.5 etc.)

c. observation = current data --> into likelihood function (e.g. dbinom...)

2. summarize the posterior (HPDI, PI)

a. estimate the posterior (e.g. with grid approximation) with Bayes' equation

b. sample from the posterior 

have a posterior curve --> then randomly sample from the posterior to approximate it

to estimate probability of parameters, want to take integrals under posterior curve BUT INTEGRALS ARE HARD

instead, just randomly sample the posterior distribution a bunch and ask where samples came from to estimate probability WITHOUT INTEGRALS

c. sampling: ask about probabilities and maximum estimates of parameters

d. describe shape of distribution

3. determine what parameters are important (loss function)
use posterior as prior in further models

a. use the model and the posterior estimates of uncertainty to make predictions

b. simulate. take other data (observations), see if model fits.

c. get a new data set (e.g. tosses of a new globe), then estimate how likely it is that you are tossing the same globe as before // how often would you have observed x?
e.g. do 2 populations differ?

## real life examples

binomial distribution: segregation ratios

expect 3:1 if dominant

how often 60:40 if truly 0.75

seed germination / survival rate

with 2 populations, is *real* germination rate the same or different?

## R bits
```{r}
head(samples, 20) #number tells head how many lines/ elements to print

```