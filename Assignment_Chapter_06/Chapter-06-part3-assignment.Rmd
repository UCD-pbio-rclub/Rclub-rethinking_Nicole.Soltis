---
title: "Chapter-06-part3-assingment"
author: "Nicole E Soltis"
date: "May 23, 2016"
output: html_document
---

## 6M2
Model selection: use information criteria to select the model with the most weight - as it will on average make better predictions.
Model averaging: preserve multiple models and uncertainty about models as determined by information criteria - avoid overconfidence in our predictions.

## 6M4 ??
What happens to the effective number of parameters, as measured by DIC or WAIC, as a prior
becomes more concentrated? Why? Perform some experiments, if you are not sure.

## 6HARD
Howell !Kung demography
data and split it into two equally sized data frames

two randomly formed data frames, each with 272 rows
use the cases in d1 to fit models and the cases in d2 to evaluate them

```{r}
library(rethinking)
data(Howell1)
d <- Howell1
d$age <- (d$age - mean(d$age))/sd(d$age)
set.seed( 1000 )
i <- sample(1:nrow(d),size=nrow(d)/2)
d1 <- d[ i , ]
d2 <- d[ -i , ]

names(d1)
#first order model
m1 <- map(
  alist(
    height ~ dnorm( mu , sigma ),
    mu <- a + b1*age,
    a ~ dnorm(mean(height),50),
    sigma ~ dunif(0,50)
  ) ,
  data=d1, start=list(b1=0))
#second order model
m2 <- map(
  alist(
   height ~ dnorm( mu , sigma ) ,
    mu <- a + b1*age + b2*(age^2),
  a ~ dnorm(mean(height),50),
    sigma ~ dunif(0,50)
  ) ,
  data=d1, start=list(b1=0, b2=0))
#third order model
m3 <- map(
  alist(
   height ~ dnorm( mu , sigma ) ,
    mu <- a + b1*age + b2*(age^2) + b3*(age^3),
  a ~ dnorm(mean(height),50),
    sigma ~ dunif(0,50)
  ) ,
  data=d1, start=list(b1=0, b2=0, b3=0))
#fourth order model
m4 <- map(
  alist(
   height ~ dnorm( mu , sigma ) ,
    mu <- a + b1*age + b2*(age^2) + b3*(age^3) + b4*(age^4),
  a ~ dnorm(mean(height),50),
    sigma ~ dunif(0,50)
  ) ,
  data=d1, start=list(b1=0, b2=0, b3=0, b4=0))
#fifth order model
m5 <- map(
  alist(
   height ~ dnorm( mu , sigma ) ,
    mu <- a + b1*age + b2*(age^2) + b3*(age^3) + b4*(age^4) + b5*(age^5),
  a ~ dnorm(mean(height),50),
    sigma ~ dunif(0,50)
  ) ,
  data=d1, start=list(b1=0, b2=0, b3=0, b4=0, b5=0))
#sixth order model
m6 <- map(
  alist(
   height ~ dnorm( mu , sigma ) ,
    mu <- a + b1*age + b2*(age^2) + b3*(age^3) + b4*(age^4) + b5*(age^5) + b6*(age^6),
  a ~ dnorm(mean(height),50),
    sigma ~ dunif(0,50)
  ) ,
  data=d1, start=list(b1=0, b2=0, b3=0, b4=0, b5=0, b6=0))

#6H1
compare(m1,m2,m3,m4,m5,m6)

#6H2
ag.seq <- seq(from=-1.45,to=2.61,length.out=30)
d.predict <- list(
  height = rep(0,30), # empty outcome
  age = ag.seq
)
#m1
pred.6h2.m1 <- link( m1 , data=d.predict )
mu.m1 <- apply( pred.6h2.m1 , 2 , mean )
mu.PI.m1 <- apply( pred.6h2.m1 , 2, function(x) PI(x, prob=0.97) )
# plot it all
plot( height ~ age , d1 , col=rangi2 )
#dashed regression line
lines( ag.seq , mu.m1 , lty=2 )
shade(mu.PI.m1, ag.seq)

#m2
pred.6h2.m2 <- link( m2 , data=d.predict )
mu.m2 <- apply( pred.6h2.m2 , 2 , mean )
mu.PI.m2 <- apply( pred.6h2.m2 , 2, function(x) PI(x, prob=0.97) )
plot( height ~ age , d1 , col=rangi2 )
lines( ag.seq , mu.m2 , lty=2 )
shade(mu.PI.m2, ag.seq)

#m3
pred.6h2.m3 <- link( m3 , data=d.predict )
mu.m3 <- apply( pred.6h2.m3 , 2 , mean )
mu.PI.m3 <- apply( pred.6h2.m3 , 2, function(x) PI(x, prob=0.97) )
plot( height ~ age , d1 , col=rangi2 )
lines( ag.seq , mu.m3 , lty=2 )
shade(mu.PI.m3, ag.seq)

#m4
pred.6h2.m4 <- link( m4 , data=d.predict )
mu.m4 <- apply( pred.6h2.m4 , 2 , mean )
mu.PI.m4 <- apply( pred.6h2.m4 , 2, function(x) PI(x, prob=0.97) )
plot( height ~ age , d1 , col=rangi2 )
lines( ag.seq , mu.m4 , lty=2 )
shade(mu.PI.m4, ag.seq)

#m5
pred.6h2.m5 <- link( m5 , data=d.predict )
mu.m5 <- apply( pred.6h2.m5 , 2 , mean )
mu.PI.m5 <- apply( pred.6h2.m5 , 2, function(x) PI(x, prob=0.97) )
plot( height ~ age , d1 , col=rangi2 )
lines( ag.seq , mu.m5 , lty=2 )
shade(mu.PI.m5, ag.seq)

#m6
pred.6h2.m6 <- link( m6 , data=d.predict )
mu.m6 <- apply( pred.6h2.m6 , 2 , mean )
mu.PI.m6 <- apply( pred.6h2.m6 , 2 , function(x) PI(x, prob=0.97) )
plot( height ~ age , d1 , col=rangi2 )
lines( ag.seq , mu.m6 , lty=2 )
shade(mu.PI.m6, ag.seq)

#6H3
d1.ensemble <- ensemble( m1 , m2 , m3 , m4 , m5, m6, data=d.predict )
mu.6h3 <- apply( d1.ensemble$link , 2 , mean )
mu.PI.6h3 <- apply( d1.ensemble$link , 2 , PI )
plot( height ~ age , d1 , col=rangi2 )
lines( ag.seq , mu.6h3 )
shade( mu.PI.6h3 , ag.seq )

#6H4
#compute test-sample deviance for each model
#calculate deviance using data in d2
#m1
post.m1 <- extract.samples(m1,n=1000)

#log-likelihood of each observation i at each sample s from the posterior
n_samples <- 1000
loglik.m1 <- sapply( 1:n_samples ,
function(s) {
mu <- post.m1$a[s] + post.m1$b1[s]*d2$age
dnorm( d2$height , mu , post.m1$sigma[s] , log=TRUE )
} )

#compute Bayesian deviance lppd
n_cases <- nrow(d2) 
lppd.m1 <- sapply( 1:n_cases , function(i) log_sum_exp(loglik.m1[i,]) - log(n_samples) )

#m2
post.m2 <- extract.samples(m2,n=1000)
n_samples <- 1000
ll.m2 <- sapply( 1:n_samples ,
function(s) {
mu <- post.m2$a[s] + post.m2$b1[s]*d2$age + post.m2$b2[s]*(d2$age^2)
dnorm( d2$height , mu , post.m2$sigma[s] , log=TRUE )
} )
n_cases <- nrow(d2) 
lppd.m2 <- sapply( 1:n_cases , function(i) log_sum_exp(ll.m2[i,]) - log(n_samples))
                   
#m3
post.m3 <- extract.samples(m3,n=1000)
n_samples <- 1000
ll.m3 <- sapply( 1:n_samples ,
function(s) {
mu <- post.m3$a[s] + post.m3$b1[s]*d2$age + post.m3$b2[s]*(d2$age^2) + post.m3$b3[s]*(d2$age^3)
dnorm( d2$height , mu , post.m3$sigma[s] , log=TRUE )
} )
n_cases <- nrow(d2) 
lppd.m3 <- sapply( 1:n_cases , function(i) log_sum_exp(ll.m3[i,]) - log(n_samples))
                   
#m4
post.m4 <- extract.samples(m4,n=1000)
n_samples <- 1000
ll.m4 <- sapply( 1:n_samples ,
function(s) {
mu <- post.m4$a[s] + post.m4$b1[s]*d2$age + post.m4$b2[s]*(d2$age^2) + post.m4$b3[s]*(d2$age^3) + post.m4$b4[s]*(d2$age^4)
dnorm( d2$height , mu , post.m4$sigma[s] , log=TRUE )
} )
n_cases <- nrow(d2) 
lppd.m4 <- sapply( 1:n_cases , function(i) log_sum_exp(ll.m4[i,]) - log(n_samples))

#m5
post.m5 <- extract.samples(m5,n=1000)
n_samples <- 1000
ll.m5 <- sapply( 1:n_samples ,
function(s) {
mu <- post.m5$a[s] + post.m5$b1[s]*d2$age + post.m5$b2[s]*(d2$age^2) + post.m5$b3[s]*(d2$age^3) + post.m5$b4[s]*(d2$age^4) + post.m5$b5[s]*(d2$age^5)
dnorm( d2$height , mu , post.m5$sigma[s] , log=TRUE )
} )
n_cases <- nrow(d2) 
lppd.m5 <- sapply( 1:n_cases , function(i) log_sum_exp(ll.m5[i,]) - log(n_samples))

#m6
post.m6 <- extract.samples(m6,n=1000)
n_samples <- 1000
ll.m6 <- sapply( 1:n_samples ,
function(s) {
mu <- post.m6$a[s] + post.m6$b1[s]*d2$age + post.m6$b2[s]*(d2$age^2) + post.m6$b3[s]*(d2$age^3) + post.m6$b4[s]*(d2$age^4) + post.m6$b5[s]*(d2$age^5) + post.m6$b6[s]*(d2$age^6)
dnorm( d2$height , mu , post.m6$sigma[s] , log=TRUE )
} )
n_cases <- nrow(d2) 
lppd.m6 <- sapply( 1:n_cases , function(i) log_sum_exp(ll.m6[i,]) - log(n_samples))

sum(lppd.m1)                   
sum(lppd.m2)
sum(lppd.m3)
sum(lppd.m4)
sum(lppd.m5)
sum(lppd.m6)

#6H5
Dev.m1 <- sum(lppd.m1) - sum(lppd.m6)
Dev.m2 <- sum(lppd.m2) - sum(lppd.m6)
Dev.m3 <- sum(lppd.m3) - sum(lppd.m6)
Dev.m4 <- sum(lppd.m4) - sum(lppd.m6)
Dev.m5 <- sum(lppd.m5) - sum(lppd.m6)
Dev.m6 <- sum(lppd.m6) - sum(lppd.m6)

WAIC.m1 <- 2395.4 - 1926.1
WAIC.m2 <- 2150.2 - 1926.1
WAIC.m3 <- 1952.7 - 1926.1
WAIC.m4 <- 1926.1 - 1926.1
WAIC.m5 <- 1927.5 - 1926.1
WAIC.m6 <- 1927.7 - 1926.1

Dev.m1; Dev.m2; Dev.m3; Dev.m4; Dev.m5; Dev.m6
WAIC.m1; WAIC.m2; WAIC.m3; WAIC.m4; WAIC.m5; WAIC.m6
```

##6H1

model rankings:  m4 > m5 = m6 > m3 > m2 > m1

WAIC weights: 
- m4 = 0.56
- m6 = 0.22
- m5 = 0.21
- m1, m2, m3 = 0

## 6H2
Model 1 and model 2 fit the data poorly; models 4 through 6 predict the data reasonably well. The PI is high over age 2.

## 6H3
The averaged predictions have a narrower PI than the highest WAIC model, particularly in the data-sparse region with high ages. There's a little less "wiggle" to the prediction (inflection?).

## 6H4
Compute the test-sample deviance for each model. This means calculating deviance, but using
the data in d2 now. 


## 6H5
Model 6 makes the best out-of-sample predictions. WAIC doesn't seem highly accurate.

## 6H6
```{r}
m6h6 <- map(
  alist(
   height ~ dnorm( mu , sigma ) ,
    mu <- a + b1*age + b2*(age^2) + b3*(age^3) + b4*(age^4) + b5*(age^5) + b6*(age^6),
  b1 ~ dnorm( 0 , 5 ) ,
  b2 ~ dnorm( 0 , 5 ) ,
  b3 ~ dnorm( 0 , 5 ) ,
  b4 ~ dnorm( 0 , 5 ) ,
  b5 ~ dnorm( 0 , 5 ) ,
  b6 ~ dnorm( 0 , 5 ) ,
  sigma ~ dunif( 0 , 50 ),
  a ~ dnorm( mean(height) , 50 )),
  data=d1 )

#plot predictions
ag.seq <- seq(from=-1.45,to=2.61,length.out=30)
d.predict <- list(
  height = rep(0,30), # empty outcome
  age = ag.seq
)
pred.m6h6 <- link(m6h6, data=d.predict)
mu.m6h6 <- apply(pred.m6h6, 2, mean)
mu.PI.m6h6 <- apply(pred.m6h6, 2, function(x) PI(x, prob=0.97))
plot(height~age, d1, col=rangi2)
lines(ag.seq, mu.m6h6, lty=2)
shade(mu.PI.m6h6, ag.seq)

#compute out-of-sample deviance
post.m6h6 <- extract.samples(m6h6,n=1000)
n_samples <- 1000
ll.m6h6 <- sapply( 1:n_samples ,
function(s) {
mu <- post.m6h6$a[s] + post.m6h6$b1[s]*d2$age + post.m6h6$b2[s]*(d2$age^2) + post.m6h6$b3[s]*(d2$age^3) + post.m6h6$b4[s]*(d2$age^4) + post.m6h6$b5[s]*(d2$age^5) + post.m6h6$b6[s]*(d2$age^6)
dnorm( d2$height , mu , post.m6h6$sigma[s] , log=TRUE )
} )
n_cases <- nrow(d2) 
lppd.m6h6 <- sapply( 1:n_cases , function(i) log_sum_exp(ll.m6h6[i,]) - log(n_samples))

sum(lppd.m6h6)  

```

slightly better than best WAIC from earlier.